{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivation of L2L3 MC corrections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from coffea import hist, processor, nanoevents\n",
    "from coffea import util\n",
    "from coffea.nanoevents.methods import candidate\n",
    "from coffea.nanoevents import BaseSchema, NanoAODSchema\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import itertools\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.behavior.update(candidate.behavior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change to your xrootd username, unless you're AC Williams ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrootdstr = 'root://acwillia@cmsxrootd.fnal.gov/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the experimental dask processor\n",
    "\n",
    "If you'd like ... \n",
    "\n",
    "`useColumnClient` should only be set to `True` if working in a singularity shell with the latest coffea-columnservice, rather than coffea-dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useDask = False\n",
    "useColumnClient = False #This option should stay false unless using coffea-columnservices in singularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if useDask:\n",
    "    if useColumnClient:\n",
    "        from columnservice.client import ColumnClient\n",
    "        cc = ColumnClient(\"coffea-dask.fnal.gov\")\n",
    "        client = cc.get_dask()\n",
    "    else:\n",
    "        from distributed import Client\n",
    "        client = Client('coffea-dask.fnal.gov:8786')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the processor, or import one from a separate `.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at ProcessorABC to see the expected methods and what they are supposed to do\n",
    "class FancyJECL2L3Processor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"Primary dataset\")\n",
    "        eta_axis = hist.Bin(\"eta\", r\"$\\eta$\", 20, -5, 5)\n",
    "        pt_axis = hist.Bin(\"pt\", r\"$p_{T}$ [GeV]\", \n",
    "                           np.array([0,5,10,15,20,25,30,35,40,45,50,60,70,80,90,\n",
    "                                     100,120,140,160,180,\n",
    "                                     200,250,300,350,400,450,500,\n",
    "                                     600,700,800,900,1000,\n",
    "                                     1500,2000,3000,4000,5000]))\n",
    "        m_axis = hist.Bin(\"m\", r\"$p_{T}$ [GeV]\", 200, 0, 500)\n",
    "        r_axis = hist.Bin(\"r\", \"RECO / GEN response\", 200, 0, 5)\n",
    "        \n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'pt':hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            'eta':hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            'r_pt_ptveta':hist.Hist(\"Counts\", dataset_axis, pt_axis, eta_axis, r_axis),\n",
    "            'r_m_ptveta':hist.Hist(\"Counts\", dataset_axis, pt_axis, eta_axis, r_axis),\n",
    "            'r_m_ptvm':hist.Hist(\"Counts\", dataset_axis, pt_axis, m_axis, r_axis),\n",
    "            'cutflow': processor.defaultdict_accumulator(int),\n",
    "        })\n",
    "    \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    def process(self, events):\n",
    "        output = self.accumulator.identity()\n",
    "\n",
    "        dataset = events.metadata['dataset']\n",
    "      \n",
    "        Jets = ak.zip({\n",
    "            \"pt\": events.Jet_pt * (1 - events.Jet_rawFactor),\n",
    "            \"eta\": events.Jet_eta,\n",
    "            \"phi\": events.Jet_phi,\n",
    "            \"mass\": events.Jet_mass,\n",
    "            \"jetId\": events.Jet_jetId,\n",
    "            \"p4\": ak.zip({\n",
    "                \"pt\": events.Jet_pt * (1 - events.Jet_rawFactor),\n",
    "                \"eta\": events.Jet_eta,\n",
    "                \"phi\": events.Jet_phi,\n",
    "                \"mass\": events.Jet_mass,\n",
    "                }, with_name=\"PtEtaPhiMLorentzVector\"),\n",
    "            })\n",
    "        \n",
    "        GenJets = ak.zip({\n",
    "            \"pt\": events.GenJet_pt,\n",
    "            \"eta\": events.GenJet_eta,\n",
    "            \"phi\": events.GenJet_phi,\n",
    "            \"mass\": events.GenJet_mass,\n",
    "            \"p4\": ak.zip({\n",
    "                \"pt\": events.GenJet_pt,\n",
    "                \"eta\": events.GenJet_eta,\n",
    "                \"phi\": events.GenJet_phi,\n",
    "                \"mass\": events.GenJet_mass,\n",
    "                }, with_name=\"PtEtaPhiMLorentzVector\"),\n",
    "        })\n",
    "        \n",
    "        \n",
    "        evtweights = events.Generator_weight\n",
    "        output['cutflow']['all events'] += ak.to_awkward0(Jets).size\n",
    "\n",
    "        jetId_cut = (Jets.jetId > 0)        \n",
    "        Jets = Jets[jetId_cut]\n",
    "        output['cutflow']['>=1 with loose id'] += ak.to_awkward0(jetId_cut).any().sum()\n",
    "        twoJets = (ak.num(Jets, axis=-1) >= 2)        \n",
    "        output['cutflow']['>=2 reco jets'] += ak.to_awkward0(twoJets).sum()\n",
    "        twoGens = (ak.num(GenJets, axis=-1) >= 2)\n",
    "        output['cutflow']['>=2 gen jets'] += ak.to_awkward0(twoGens).sum()\n",
    "        \n",
    "        Jets = Jets[twoJets & twoGens]\n",
    "        GenJets = GenJets[twoJets & twoGens]\n",
    "        \n",
    "        \n",
    "        dphi_index = Jets.p4[:,0].delta_phi( Jets.p4[:,1] ) > 1.8\n",
    "        output['cutflow']['dPhi > 1.8'] += ak.to_awkward0(dphi_index).sum()\n",
    "        \n",
    "\n",
    "        Jets = Jets[dphi_index]\n",
    "        GenJets = GenJets[dphi_index]\n",
    "        \n",
    "        #pairing = Jets.p4[:,0:2].cross(GenJets.p4, nested=True)\n",
    "        pairing = ak.cartesian([Jets.p4[:,0:2], GenJets.p4])\n",
    "        metric = pairing.slot0.delta_r(pairing.slot1)\n",
    "        index_of_minimized = ak.argmin(metric, axis=-1)\n",
    "        \n",
    "        dr_cut = (metric[index_of_minimized] < 0.2)\n",
    "        best_pairings_that_pass_dr_cut = pairing[index_of_minimized][dr_cut]\n",
    "        genrecos = ak.flatten(best_pairings_that_pass_dr_cut, axis=1) #.flatten(axis=1)\n",
    "        ptresponse = genrecos.slot0.pt / genrecos.slot1.pt\n",
    "        \n",
    "        output['pt'].fill(dataset=dataset,\n",
    "                            pt=ak.flatten(Jets.pt))\n",
    "        output['eta'].fill(dataset=dataset, \n",
    "                                 eta=ak.flatten(Jets.eta))\n",
    "        output['r_pt_ptveta'].fill( dataset=dataset, pt=genrecos.slot1.pt, eta=genrecos.slot1.eta, r=ptresponse)\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the root files and store them in a dictionary `fileset` to be used in the uproot job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sample metadata\n",
    "samples = json.load( open('samples_qcdflat.json') )\n",
    "\n",
    "fileset = {}\n",
    "for sample in samples[\"samples\"]:    \n",
    "    name, xsec, nevents, files = sample['name'], sample['xsec'], sample['nevents'], sample['files']\n",
    "    for ifile,file in enumerate(files):\n",
    "        files[ifile] = xrootdstr + file\n",
    "    fileset[name] = files\n",
    "\n",
    "print(fileset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstart = time.time()\n",
    "\n",
    "if not useDask:\n",
    "    output = processor.run_uproot_job(fileset,\n",
    "                                      treename='Events',\n",
    "                                      processor_instance=FancyJECL2L3Processor(),\n",
    "                                      executor=processor.iterative_executor,\n",
    "                                      executor_args={\n",
    "                                          'skipbadfiles':False,\n",
    "                                          'schema': BaseSchema, #NanoAODSchema, \n",
    "                                          'workers': 4},\n",
    "                                      chunksize=50000, maxchunks=100\n",
    "                                     )\n",
    "else:\n",
    "    output = processor.run_uproot_job(fileset,\n",
    "                                      treename='Events',\n",
    "                                      processor_instance=FancyJECL2L3Processor(),\n",
    "                                      executor=processor.dask_executor,\n",
    "                                      executor_args={\n",
    "                                          'skipbadfiles':True,\n",
    "                                          'client': client, \n",
    "                                          'schema': BaseSchema, #NanoAODSchema, \n",
    "                                          'workers': 2}\n",
    "                                     )\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Events/s:\", output['cutflow']['all events']/elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"image.cmap\"] = 'Blues'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = hist.plotgrid(output['pt'], overlay=\"dataset\", stack=False, density=True)\n",
    "\n",
    "for iax in ax.flatten():\n",
    "    iax.set_yscale('log')\n",
    "    iax.autoscale(axis='y')\n",
    "    iax.set_title(r'Jet $p_T$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = hist.plotgrid(output['eta'], overlay=\"dataset\", stack=False, density=True)\n",
    "\n",
    "for iax in ax.flatten():\n",
    "    iax.autoscale(axis='y')\n",
    "    iax.set_title(r'Jet $\\eta$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in output['r_pt_ptveta'].axis('eta'):\n",
    "    title = r'$\\eta$ range ' + str(i)\n",
    "    ax = hist.plot2d(output['r_pt_ptveta'].sum('dataset').integrate('eta', i),xaxis='pt')\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in output['r_pt_ptveta'].axis('pt'):\n",
    "    title = r'$p_T$ range ' + str(i)\n",
    "    ax = hist.plot2d(output['r_pt_ptveta'].sum('dataset').integrate('pt',i),xaxis='eta')\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['r_pt_ptveta'].axis('eta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output['eta'].values()[('QCDFlat',)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
